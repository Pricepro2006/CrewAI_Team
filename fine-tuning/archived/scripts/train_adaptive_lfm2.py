#!/usr/bin/env python3
"""
Adaptive Training Script for LFM2-1.2B
Uses the zero-hardcoding adaptive datasets generated by Phase 2
Maps email batch numbers to Claude's comprehensive analysis
"""

import os
import json
import torch
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List
import numpy as np

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('adaptive_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def load_adaptive_dataset(dataset_path: str) -> Dict:
    """Load the adaptive dataset generated by Phase 2"""
    with open(dataset_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"Loaded dataset from {dataset_path}")
    logger.info(f"  - Total examples: {data['metadata']['total_examples']}")
    logger.info(f"  - Curriculum level: {data['metadata']['curriculum_level']}")
    logger.info(f"  - Avg difficulty: {data['metadata']['avg_difficulty']:.2f}")
    logger.info(f"  - Quality distribution: {data['metadata']['quality_distribution']}")
    
    return data

def format_for_lfm2(example: Dict) -> str:
    """Format a single example for LFM2 training"""
    # LFM2 expects a specific format for instruction tuning
    formatted = f"""### Instruction:
{example['instruction']}

### Input:
{example['input']}

### Response:
{example['output']}"""
    return formatted

def prepare_datasets():
    """Load and prepare the adaptive datasets"""
    train_path = "./datasets/adaptive_train.json"
    val_path = "./datasets/adaptive_val.json"
    
    # Load datasets
    train_data = load_adaptive_dataset(train_path)
    val_data = load_adaptive_dataset(val_path)
    
    # Extract examples
    train_examples = train_data['examples']
    val_examples = val_data['examples']
    
    logger.info(f"\n{'='*60}")
    logger.info("DATASET STATISTICS")
    logger.info(f"{'='*60}")
    logger.info(f"Training examples: {len(train_examples)}")
    logger.info(f"Validation examples: {len(val_examples)}")
    
    # Calculate unique batches
    train_batches = set(e['batch_number'] for e in train_examples)
    val_batches = set(e['batch_number'] for e in val_examples)
    logger.info(f"Unique training batches: {len(train_batches)}")
    logger.info(f"Unique validation batches: {len(val_batches)}")
    logger.info(f"Overlap: {len(train_batches & val_batches)} batches")
    
    return train_examples, val_examples

def train_with_adaptive_curriculum(train_examples: List[Dict], val_examples: List[Dict]):
    """Train LFM2 using the adaptive curriculum dataset"""
    
    logger.info(f"\n{'='*60}")
    logger.info("STARTING ADAPTIVE TRAINING")
    logger.info(f"{'='*60}")
    
    # Import training libraries
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        TrainingArguments,
        Trainer,
        DataCollatorForLanguageModeling
    )
    from peft import LoraConfig, get_peft_model, TaskType
    import datasets
    
    # Model configuration
    model_name = "LiquidAI/LFM2-1.2B"
    logger.info(f"Loading model: {model_name}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    # Load model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float32,  # CPU training
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    
    # Configure LoRA for efficient training
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        r=8,  # Rank
        lora_alpha=16,
        lora_dropout=0.05,
        target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
    )
    
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # Tokenize datasets
    def tokenize_function(examples):
        # Format each example
        texts = [format_for_lfm2(ex) for ex in examples]
        
        # Tokenize
        result = tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=2048,  # Increased for longer analyses
            return_tensors=None
        )
        
        # Set labels same as input_ids for causal LM
        result["labels"] = result["input_ids"].copy()
        return result
    
    # Create HuggingFace datasets
    train_dataset = datasets.Dataset.from_list(train_examples)
    val_dataset = datasets.Dataset.from_list(val_examples)
    
    # Apply tokenization
    logger.info("Tokenizing datasets...")
    train_dataset = train_dataset.map(
        lambda x: tokenize_function([x]),
        batched=False,
        remove_columns=train_dataset.column_names
    )
    val_dataset = val_dataset.map(
        lambda x: tokenize_function([x]),
        batched=False,
        remove_columns=val_dataset.column_names
    )
    
    # Training arguments optimized for CPU
    training_args = TrainingArguments(
        output_dir="./models/lfm2_adaptive_curriculum",
        num_train_epochs=5,
        per_device_train_batch_size=1,  # Small batch for CPU
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,  # Effective batch size of 8
        warmup_steps=50,
        logging_steps=10,
        save_steps=100,
        eval_strategy="steps",  # Changed from evaluation_strategy
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        learning_rate=3e-4,
        weight_decay=0.01,
        logging_dir='./logs',
        report_to=[],  # Removed tensorboard requirement
        fp16=False,  # No mixed precision on CPU
        dataloader_pin_memory=False,  # CPU doesn't use pinned memory
        gradient_checkpointing=False,  # Disabled - incompatible with LFM2 conv1d
        remove_unused_columns=False,
        save_total_limit=3,  # Keep only 3 checkpoints
    )
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        data_collator=DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
        ),
    )
    
    # Log training start
    logger.info(f"Training on {len(train_dataset)} examples")
    logger.info(f"Evaluating on {len(val_dataset)} examples")
    logger.info(f"Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    logger.info(f"Total training steps: {len(train_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}")
    
    # Start training
    logger.info("\nStarting training...")
    start_time = datetime.now()
    
    try:
        # Train
        train_result = trainer.train()
        
        # Log results
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info(f"\n{'='*60}")
        logger.info("TRAINING COMPLETED SUCCESSFULLY")
        logger.info(f"{'='*60}")
        logger.info(f"Duration: {duration}")
        logger.info(f"Final train loss: {train_result.training_loss:.4f}")
        logger.info(f"Total steps: {train_result.global_step}")
        
        # Save the model
        logger.info("\nSaving final model...")
        trainer.save_model("./models/lfm2_adaptive_final")
        tokenizer.save_pretrained("./models/lfm2_adaptive_final")
        
        # Evaluate
        logger.info("\nRunning final evaluation...")
        eval_results = trainer.evaluate()
        logger.info(f"Evaluation loss: {eval_results['eval_loss']:.4f}")
        
        # Save training metrics
        metrics = {
            "training_duration": str(duration),
            "final_train_loss": train_result.training_loss,
            "eval_loss": eval_results['eval_loss'],
            "total_steps": train_result.global_step,
            "train_examples": len(train_dataset),
            "val_examples": len(val_dataset),
            "model": model_name,
            "timestamp": datetime.now().isoformat()
        }
        
        with open("./models/lfm2_adaptive_final/training_metrics.json", 'w') as f:
            json.dump(metrics, f, indent=2)
        
        logger.info("\nTraining metrics saved to training_metrics.json")
        
        return trainer, eval_results
        
    except Exception as e:
        logger.error(f"Training failed: {str(e)}")
        raise

def test_model_inference(model_path: str = "./models/lfm2_adaptive_final"):
    """Test the trained model with a few examples"""
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    
    logger.info(f"\n{'='*60}")
    logger.info("TESTING MODEL INFERENCE")
    logger.info(f"{'='*60}")
    
    # Load base model
    base_model = AutoModelForCausalLM.from_pretrained(
        "LiquidAI/LFM2-1.2B",
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )
    
    # Load LoRA weights
    model = PeftModel.from_pretrained(base_model, model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # Test prompts
    test_batches = [1, 100, 500, 1000, 2000]
    
    for batch_num in test_batches:
        prompt = f"""### Instruction:
Analyze email batch #{batch_num}

### Input:
This batch contains emails from TD SYNNEX communications.

### Response:"""
        
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                top_p=0.95
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        logger.info(f"\nBatch {batch_num} response preview:")
        logger.info(response[:500] + "..." if len(response) > 500 else response)

def main():
    """Main training pipeline"""
    
    logger.info("="*80)
    logger.info("ADAPTIVE LFM2-1.2B TRAINING PIPELINE")
    logger.info("Using Phase 2 Zero-Hardcoding Datasets")
    logger.info("="*80)
    
    # Check if datasets exist
    if not Path("./datasets/adaptive_train.json").exists():
        logger.error("Adaptive datasets not found! Run phase2_adaptive_dataset_generator.py first.")
        return
    
    # Load and prepare datasets
    train_examples, val_examples = prepare_datasets()
    
    # Train with adaptive curriculum
    trainer, eval_results = train_with_adaptive_curriculum(train_examples, val_examples)
    
    # Test inference
    test_model_inference()
    
    logger.info("\n" + "="*80)
    logger.info("ADAPTIVE TRAINING PIPELINE COMPLETE")
    logger.info("="*80)
    logger.info("Model saved to: ./models/lfm2_adaptive_final")
    logger.info("Logs saved to: adaptive_training.log")
    logger.info("Metrics saved to: ./models/lfm2_adaptive_final/training_metrics.json")

if __name__ == "__main__":
    main()