[Unit]
Description=Ollama Local LLM Service
Documentation=https://github.com/ollama/ollama
After=network.target

[Service]
Type=simple
User=pricepro2006
WorkingDirectory=/home/pricepro2006/CrewAI_Team
Environment=OLLAMA_HOST=127.0.0.1:11434
Environment=OLLAMA_ORIGINS="http://localhost:*,http://127.0.0.1:*"
ExecStart=/usr/local/bin/ollama serve
ExecReload=/bin/kill -HUP $MAINPID
Restart=on-failure
RestartSec=3
StandardOutput=journal
StandardError=journal

# Resource limits (prevent memory issues)
MemoryMax=4G
CPUQuota=400%

[Install]
WantedBy=multi-user.target