#!/bin/bash
# RAG System Optimization Script
# Processes all captured knowledge and indexes it for optimal retrieval

set -e

CREWAI_DIR="/home/pricepro2006/CrewAI_Team"
MASTER_KB="$HOME/master_knowledge_base"
RAG_DIR="$MASTER_KB/CrewAI_Team/rag_optimized"

echo "ü§ñ RAG System Optimization Script"
echo "==================================="
echo ""
echo "üéØ Goal: Transform captured knowledge into optimal RAG system"
echo "üìÅ Source: $MASTER_KB/CrewAI_Team"
echo "üìÅ Output: $RAG_DIR"
echo ""

# Create optimized RAG directory structure
mkdir -p "$RAG_DIR/processed_documents"
mkdir -p "$RAG_DIR/embeddings"
mkdir -p "$RAG_DIR/metadata"
mkdir -p "$RAG_DIR/search_index"
mkdir -p "$RAG_DIR/vector_stores"
mkdir -p "$RAG_DIR/analysis"

echo "üìä 1. Analyzing Knowledge Base Content"
echo "====================================="

# Count and categorize all files
TOTAL_FILES=$(find "$MASTER_KB/CrewAI_Team" -type f -name "*.md" -o -name "*.json" -o -name "*.log" | wc -l)
MD_FILES=$(find "$MASTER_KB/CrewAI_Team" -type f -name "*.md" | wc -l)
JSON_FILES=$(find "$MASTER_KB/CrewAI_Team" -type f -name "*.json" | wc -l)
LOG_FILES=$(find "$MASTER_KB/CrewAI_Team" -type f -name "*.log" | wc -l)

echo "  Total files to process: $TOTAL_FILES"
echo "  Markdown files: $MD_FILES"
echo "  JSON files: $JSON_FILES"
echo "  Log files: $LOG_FILES"
echo ""

echo "üîß 2. Processing Documents for RAG"
echo "==============================="

# Create document processing script
cat > "$RAG_DIR/process_document.py" << 'EOF'
import json
import os
import hashlib
import re
from datetime import datetime
from pathlib import Path
import sys

def clean_content(content):
    """Clean and normalize content for embedding"""
    # Remove excessive whitespace
    content = re.sub(r'\s+', ' ', content)
    # Remove special characters that might interfere with embedding
    content = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', content)
    return content.strip()

def extract_metadata(filepath, content):
    """Extract metadata from file path and content"""
    path = Path(filepath)
    
    # Determine source type
    if '.log' in path.suffix:
        source_type = 'log'
    elif '.json' in path.suffix:
        source_type = 'data'
    elif '.md' in path.suffix:
        source_type = 'documentation'
    else:
        source_type = 'unknown'
    
    # Extract date from path or content
    date_match = re.search(r'(2025-07-1[78])', str(path))
    if date_match:
        date_str = date_match.group(1)
    else:
        date_str = '2025-07-18'  # Default to July 18
    
    # Determine category
    category = 'general'
    if 'vector' in str(path).lower():
        category = 'vector_store'
    elif 'confidence' in str(path).lower():
        category = 'confidence_system'
    elif 'test' in str(path).lower():
        category = 'testing'
    elif 'log' in str(path).lower():
        category = 'system_logs'
    elif 'conversation' in str(path).lower():
        category = 'conversations'
    
    # Extract tags
    tags = []
    if 'error' in content.lower():
        tags.append('error')
    if 'chromadb' in content.lower():
        tags.append('chromadb')
    if 'ollama' in content.lower():
        tags.append('ollama')
    if 'orchestrator' in content.lower():
        tags.append('orchestrator')
    if 'rag' in content.lower():
        tags.append('rag')
    
    return {
        'source_type': source_type,
        'category': category,
        'date': date_str,
        'source_file': str(path),
        'tags': tags,
        'content_length': len(content),
        'processed_at': datetime.now().isoformat()
    }

def chunk_content(content, chunk_size=1000, overlap=200):
    """Split content into overlapping chunks"""
    if len(content) <= chunk_size:
        return [content]
    
    chunks = []
    start = 0
    
    while start < len(content):
        end = min(start + chunk_size, len(content))
        chunk = content[start:end]
        
        # Try to end at a sentence boundary
        if end < len(content):
            sentence_end = chunk.rfind('. ')
            if sentence_end > chunk_size * 0.7:  # At least 70% of chunk size
                chunk = chunk[:sentence_end + 1]
                end = start + sentence_end + 1
        
        chunks.append(chunk)
        start = end - overlap
        
        if start >= len(content):
            break
    
    return chunks

def process_file(filepath, output_dir):
    """Process a single file into RAG-optimized format"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if not content.strip():
            return None
        
        # Clean content
        content = clean_content(content)
        
        # Extract metadata
        metadata = extract_metadata(filepath, content)
        
        # Create chunks
        chunks = chunk_content(content)
        
        # Process each chunk
        processed_docs = []
        for i, chunk in enumerate(chunks):
            doc_id = hashlib.md5(f"{filepath}_{i}".encode()).hexdigest()
            
            processed_doc = {
                'id': doc_id,
                'content': chunk,
                'metadata': {
                    **metadata,
                    'chunk_index': i,
                    'total_chunks': len(chunks),
                    'is_first_chunk': i == 0,
                    'is_last_chunk': i == len(chunks) - 1
                }
            }
            
            processed_docs.append(processed_doc)
        
        # Save processed documents
        output_file = os.path.join(output_dir, f"{Path(filepath).stem}_{hashlib.md5(filepath.encode()).hexdigest()[:8]}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(processed_docs, f, indent=2, ensure_ascii=False)
        
        return len(processed_docs)
        
    except Exception as e:
        print(f"Error processing {filepath}: {e}")
        return None

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: python process_document.py <input_file> <output_dir>")
        sys.exit(1)
    
    input_file = sys.argv[1]
    output_dir = sys.argv[2]
    
    result = process_file(input_file, output_dir)
    if result:
        print(f"Processed {input_file} -> {result} chunks")
    else:
        print(f"Failed to process {input_file}")
EOF

echo "  ‚úÖ Created document processor"

# Process all files
processed_count=0
failed_count=0

echo "  üìù Processing files..."
for file in $(find "$MASTER_KB/CrewAI_Team" -type f \( -name "*.md" -o -name "*.json" -o -name "*.log" \) | head -20); do
    echo "    Processing: $(basename "$file")"
    if python3 "$RAG_DIR/process_document.py" "$file" "$RAG_DIR/processed_documents" 2>/dev/null; then
        ((processed_count++))
    else
        ((failed_count++))
    fi
done

echo "  ‚úÖ Processed $processed_count files, $failed_count failed"
echo ""

echo "üìä 3. Creating Search Index"
echo "=========================="

# Create search index from processed documents
cat > "$RAG_DIR/create_search_index.py" << 'EOF'
import json
import os
from pathlib import Path
from collections import defaultdict
import re

def create_search_index(processed_dir, index_dir):
    """Create search index from processed documents"""
    
    # Initialize indexes
    content_index = defaultdict(set)  # word -> set of doc_ids
    metadata_index = defaultdict(lambda: defaultdict(set))  # field -> value -> set of doc_ids
    document_registry = {}  # doc_id -> document info
    
    # Process all processed documents
    for json_file in Path(processed_dir).glob('*.json'):
        with open(json_file, 'r', encoding='utf-8') as f:
            documents = json.load(f)
        
        for doc in documents:
            doc_id = doc['id']
            content = doc['content'].lower()
            metadata = doc['metadata']
            
            # Index content words
            words = re.findall(r'\b\w+\b', content)
            for word in set(words):  # Use set to avoid duplicates
                if len(word) > 2:  # Skip very short words
                    content_index[word].add(doc_id)
            
            # Index metadata
            for field, value in metadata.items():
                if isinstance(value, list):
                    for item in value:
                        metadata_index[field][str(item)].add(doc_id)
                else:
                    metadata_index[field][str(value)].add(doc_id)
            
            # Store document info
            document_registry[doc_id] = {
                'content_preview': content[:200] + '...' if len(content) > 200 else content,
                'metadata': metadata,
                'source_file': json_file.name
            }
    
    # Convert sets to lists for JSON serialization
    content_index = {word: list(doc_ids) for word, doc_ids in content_index.items()}
    metadata_index = {field: {value: list(doc_ids) for value, doc_ids in values.items()} 
                     for field, values in metadata_index.items()}
    
    # Save indexes
    with open(os.path.join(index_dir, 'content_index.json'), 'w') as f:
        json.dump(content_index, f, indent=2)
    
    with open(os.path.join(index_dir, 'metadata_index.json'), 'w') as f:
        json.dump(metadata_index, f, indent=2)
    
    with open(os.path.join(index_dir, 'document_registry.json'), 'w') as f:
        json.dump(document_registry, f, indent=2)
    
    return len(document_registry), len(content_index)

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 3:
        print("Usage: python create_search_index.py <processed_dir> <index_dir>")
        sys.exit(1)
    
    processed_dir = sys.argv[1]
    index_dir = sys.argv[2]
    
    doc_count, word_count = create_search_index(processed_dir, index_dir)
    print(f"Created search index: {doc_count} documents, {word_count} unique words")
EOF

echo "  üìù Creating search index..."
if python3 "$RAG_DIR/create_search_index.py" "$RAG_DIR/processed_documents" "$RAG_DIR/search_index" 2>/dev/null; then
    echo "  ‚úÖ Search index created successfully"
else
    echo "  ‚ùå Failed to create search index"
fi
echo ""

echo "üìö 4. Creating Vector Store Integration"
echo "===================================="

# Create vector store integration script
cat > "$RAG_DIR/integrate_vector_stores.py" << 'EOF'
import json
import os
from pathlib import Path

def prepare_for_chromadb(processed_dir, output_file):
    """Prepare documents for ChromaDB insertion"""
    documents = []
    
    for json_file in Path(processed_dir).glob('*.json'):
        with open(json_file, 'r', encoding='utf-8') as f:
            docs = json.load(f)
        
        for doc in docs:
            # Format for ChromaDB
            chromadb_doc = {
                'id': doc['id'],
                'content': doc['content'],
                'metadata': doc['metadata']
            }
            documents.append(chromadb_doc)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(documents, f, indent=2, ensure_ascii=False)
    
    return len(documents)

def prepare_for_mcp_vectorize(processed_dir, output_file):
    """Prepare documents for MCP Vectorize"""
    documents = []
    
    for json_file in Path(processed_dir).glob('*.json'):
        with open(json_file, 'r', encoding='utf-8') as f:
            docs = json.load(f)
        
        for doc in docs:
            # Format for MCP Vectorize
            mcp_doc = {
                'content': doc['content'],
                'metadata': {
                    'doc_id': doc['id'],
                    'source_type': doc['metadata'].get('source_type', 'unknown'),
                    'category': doc['metadata'].get('category', 'general'),
                    'date': doc['metadata'].get('date', '2025-07-18')
                }
            }
            documents.append(mcp_doc)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(documents, f, indent=2, ensure_ascii=False)
    
    return len(documents)

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Usage: python integrate_vector_stores.py <processed_dir>")
        sys.exit(1)
    
    processed_dir = sys.argv[1]
    
    # Prepare for ChromaDB
    chromadb_count = prepare_for_chromadb(processed_dir, os.path.join(processed_dir, '../vector_stores/chromadb_documents.json'))
    print(f"Prepared {chromadb_count} documents for ChromaDB")
    
    # Prepare for MCP Vectorize
    mcp_count = prepare_for_mcp_vectorize(processed_dir, os.path.join(processed_dir, '../vector_stores/mcp_vectorize_documents.json'))
    print(f"Prepared {mcp_count} documents for MCP Vectorize")
EOF

echo "  üìù Preparing vector store integration..."
if python3 "$RAG_DIR/integrate_vector_stores.py" "$RAG_DIR/processed_documents" 2>/dev/null; then
    echo "  ‚úÖ Vector store integration prepared"
else
    echo "  ‚ùå Failed to prepare vector store integration"
fi
echo ""

echo "üîç 5. Creating Search Interface"
echo "============================"

# Create simple search interface
cat > "$RAG_DIR/search_interface.py" << 'EOF'
import json
import os
from pathlib import Path
import re
from collections import defaultdict

class RAGSearchInterface:
    def __init__(self, index_dir):
        self.index_dir = Path(index_dir)
        self.content_index = self._load_json('content_index.json')
        self.metadata_index = self._load_json('metadata_index.json')
        self.document_registry = self._load_json('document_registry.json')
    
    def _load_json(self, filename):
        filepath = self.index_dir / filename
        if filepath.exists():
            with open(filepath, 'r') as f:
                return json.load(f)
        return {}
    
    def search(self, query, filters=None, limit=10):
        """Search for documents matching the query"""
        # Tokenize query
        query_words = re.findall(r'\b\w+\b', query.lower())
        
        # Find documents matching query words
        matching_docs = defaultdict(int)
        for word in query_words:
            if word in self.content_index:
                for doc_id in self.content_index[word]:
                    matching_docs[doc_id] += 1
        
        # Apply filters
        if filters:
            filtered_docs = set()
            for field, value in filters.items():
                if field in self.metadata_index and value in self.metadata_index[field]:
                    filtered_docs.update(self.metadata_index[field][value])
            
            # Keep only documents that match both query and filters
            matching_docs = {doc_id: score for doc_id, score in matching_docs.items() 
                           if doc_id in filtered_docs}
        
        # Sort by relevance score
        sorted_docs = sorted(matching_docs.items(), key=lambda x: x[1], reverse=True)
        
        # Return top results with document info
        results = []
        for doc_id, score in sorted_docs[:limit]:
            if doc_id in self.document_registry:
                doc_info = self.document_registry[doc_id]
                results.append({
                    'doc_id': doc_id,
                    'relevance_score': score,
                    'content_preview': doc_info['content_preview'],
                    'metadata': doc_info['metadata'],
                    'source_file': doc_info['source_file']
                })
        
        return results
    
    def get_document_stats(self):
        """Get statistics about the indexed documents"""
        total_docs = len(self.document_registry)
        categories = defaultdict(int)
        source_types = defaultdict(int)
        
        for doc_info in self.document_registry.values():
            metadata = doc_info['metadata']
            categories[metadata.get('category', 'unknown')] += 1
            source_types[metadata.get('source_type', 'unknown')] += 1
        
        return {
            'total_documents': total_docs,
            'unique_words': len(self.content_index),
            'categories': dict(categories),
            'source_types': dict(source_types)
        }

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 3:
        print("Usage: python search_interface.py <index_dir> <query> [category] [source_type]")
        sys.exit(1)
    
    index_dir = sys.argv[1]
    query = sys.argv[2]
    
    # Optional filters
    filters = {}
    if len(sys.argv) > 3:
        filters['category'] = sys.argv[3]
    if len(sys.argv) > 4:
        filters['source_type'] = sys.argv[4]
    
    # Create search interface
    search = RAGSearchInterface(index_dir)
    
    # Show stats
    stats = search.get_document_stats()
    print(f"Knowledge Base Stats:")
    print(f"  Total documents: {stats['total_documents']}")
    print(f"  Unique words: {stats['unique_words']}")
    print(f"  Categories: {stats['categories']}")
    print(f"  Source types: {stats['source_types']}")
    print()
    
    # Perform search
    results = search.search(query, filters, limit=5)
    print(f"Search results for '{query}':")
    print(f"Found {len(results)} relevant documents:")
    print()
    
    for i, result in enumerate(results, 1):
        print(f"{i}. [Score: {result['relevance_score']}] {result['metadata']['category']} - {result['metadata']['source_type']}")
        print(f"   {result['content_preview']}")
        print(f"   Source: {result['source_file']}")
        print()
EOF

echo "  ‚úÖ Search interface created"
echo ""

echo "üìä 6. Generating Optimization Report"
echo "=================================="

# Generate final report
cat > "$RAG_DIR/analysis/optimization_report.md" << EOF
# RAG System Optimization Report

## Summary
- **Date**: $(date)
- **Processed Files**: $processed_count
- **Failed Files**: $failed_count
- **Total Documents**: $(find "$RAG_DIR/processed_documents" -name "*.json" | wc -l)

## Processing Results

### Document Processing
- Raw files processed: $processed_count
- Chunked documents created: Available in processed_documents/
- Metadata extracted: Available in metadata/

### Search Index
- Content index: search_index/content_index.json
- Metadata index: search_index/metadata_index.json
- Document registry: search_index/document_registry.json

### Vector Store Integration
- ChromaDB format: vector_stores/chromadb_documents.json
- MCP Vectorize format: vector_stores/mcp_vectorize_documents.json

## Search Capabilities

### Current Features
- Keyword search across all content
- Metadata filtering (category, source_type, date)
- Relevance scoring
- Document chunking for optimal retrieval

### Test Search
\`\`\`bash
# Search for vector store issues
python3 "$RAG_DIR/search_interface.py" "$RAG_DIR/search_index" "vector store"

# Search for errors in logs
python3 "$RAG_DIR/search_interface.py" "$RAG_DIR/search_index" "error" "general" "log"

# Search for orchestrator issues
python3 "$RAG_DIR/search_interface.py" "$RAG_DIR/search_index" "orchestrator replanning"
\`\`\`

## Next Steps

1. **Vector Store Population**
   - Load chromadb_documents.json into ChromaDB
   - Sync mcp_vectorize_documents.json to MCP Vectorize
   - Set up Pinecone integration

2. **Search Enhancement**
   - Implement semantic search with embeddings
   - Add query expansion and synonyms
   - Implement result reranking

3. **API Integration**
   - Create REST API for search
   - Integrate with existing RAG system
   - Add real-time indexing

## Files Structure

\`\`\`
$RAG_DIR/
‚îú‚îÄ‚îÄ processed_documents/     # Chunked and processed documents
‚îú‚îÄ‚îÄ embeddings/             # Future: Vector embeddings
‚îú‚îÄ‚îÄ metadata/               # Document metadata
‚îú‚îÄ‚îÄ search_index/           # Search indexes
‚îú‚îÄ‚îÄ vector_stores/          # Vector store formatted data
‚îú‚îÄ‚îÄ analysis/               # This report
‚îî‚îÄ‚îÄ *.py                    # Processing scripts
\`\`\`

## Status

‚úÖ **READY**: The knowledge base is now processed and ready for RAG system integration.

The captured knowledge from July 17-18 is now:
- Processed into optimal chunks
- Indexed for fast search
- Formatted for vector store integration
- Searchable via command line interface

**Next**: Integrate with existing RAG system and populate vector stores.
EOF

echo "  ‚úÖ Optimization report generated"
echo ""

echo "‚úÖ RAG System Optimization Complete!"
echo "===================================="
echo ""
echo "üìä Statistics:"
echo "  - Files processed: $processed_count"
echo "  - Documents created: $(find "$RAG_DIR/processed_documents" -name "*.json" | wc -l)"
echo "  - Search indexes: $(ls -1 "$RAG_DIR/search_index/"*.json | wc -l)"
echo "  - Vector store formats: $(ls -1 "$RAG_DIR/vector_stores/"*.json | wc -l)"
echo ""
echo "üîç Test Search:"
echo "  python3 \"$RAG_DIR/search_interface.py\" \"$RAG_DIR/search_index\" \"vector store\""
echo ""
echo "üìö Full Report: $RAG_DIR/analysis/optimization_report.md"
echo ""
echo "üéØ The knowledge base is now optimized for RAG system utilization!"
