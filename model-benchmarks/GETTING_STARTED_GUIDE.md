# TD SYNNEX Pattern Extraction - Getting Started Guide

## System Overview
You now have a powerful pattern extraction system that has already discovered **147,685 unique patterns** from your 143,221 emails. The system can extract business entities with 85.2% coverage (vs 1.4% before).

---

## üöÄ Quick Start (5 Minutes)

### Step 1: Test Basic Extraction
```python
# Quick test script
from production_hybrid_extractor import ProductionHybridExtractor

extractor = ProductionHybridExtractor()
result = extractor.extract_entities("""
    Quote WQ1234567890 converted to PO 0505915850
    Apply SPA CAS-107073-B4P8K8
""")

print(f"Found {len(result.entities)} entities")
for e in result.entities:
    print(f"  - {e.value} ({e.type})")
```

### Step 2: Process Your Own Emails
```python
# Process a real email
email_text = """
[Paste your TD SYNNEX email here]
"""

result = extractor.extract_entities(email_text)
print(f"Purpose: {result.purpose}")
print(f"Workflow: {result.workflow}")
```

---

## üìä What You Can Do Now

### 1. **Extract Patterns from Any Email**
The system automatically finds and classifies:
- Quotes (WQ, FTQ, CPQ formats)
- Purchase Orders (with leading zeros support)
- SPAs (CAS, US_COM, SP patterns)
- Tickets (TS, TASK, INC)
- Deal Registrations
- Project Codes
- Customer IDs
- Internal References
- Plus 147,685 discovered patterns!

### 2. **Understand Email Purpose**
The system classifies emails into:
- `quote_request` - Pricing and quote requests
- `order_processing` - PO and order handling
- `support_ticket` - Support issues
- `contract_update` - SPA updates
- `shipment_tracking` - Delivery tracking
- `invoice` - Billing matters

### 3. **Detect Workflows**
Automatically identifies:
- Quote-to-Order conversions
- SPA order processing
- Support escalations
- Shipment tracking flows

---

## üéØ Recommended Next Steps

### Priority 1: Review Discovered Patterns (15 minutes)
Look at what was found in your emails:

```bash
# View the discovery report
cat /home/pricepro2006/CrewAI_Team/model-benchmarks/full_discovery/final_report_20250809_224330.txt | head -100

# See top patterns by category
grep "Top 10:" -A 10 full_discovery/final_report_20250809_224330.txt
```

**Key Findings to Review:**
- `sales4460` appears 6,064 times - is this a system ID?
- `GENERATED:57371` appears 5,084 times - automated system?
- Location codes (SC 29615, CA 94538) - customer locations?
- CAS patterns confirmed as SPAs ‚úÖ

### Priority 2: Human Verification (30 minutes)
Help the system learn by verifying patterns:

```bash
python3 human_verification_interface.py
```

This will show you patterns and ask you to classify them:
1. Quote
2. Purchase Order
3. SPA
4. Ticket
5. Deal
6. Project
7. Customer ID
8. Internal Reference
9. Other

The more you verify, the smarter the system becomes!

### Priority 3: Test with Real Emails (10 minutes)
Create a test file with your actual emails:

```python
# test_real_emails.py
from production_hybrid_extractor import ProductionHybridExtractor
import json

extractor = ProductionHybridExtractor()

# Your real emails
emails = [
    """[Paste email 1 here]""",
    """[Paste email 2 here]""",
    """[Paste email 3 here]"""
]

for i, email in enumerate(emails, 1):
    print(f"\n{'='*50}")
    print(f"Email {i}")
    print('='*50)
    
    result = extractor.extract_entities(email, f"real_email_{i}")
    
    print(f"Purpose: {result.purpose}")
    print(f"Entities found: {len(result.entities)}")
    
    # Group by type
    by_type = {}
    for entity in result.entities:
        by_type.setdefault(entity.classification or entity.type, []).append(entity.value)
    
    for entity_type, values in by_type.items():
        print(f"\n{entity_type}:")
        for value in values[:5]:  # Show first 5
            print(f"  - {value}")
```

### Priority 4: Integrate with Your Workflow (1 hour)

#### Option A: API Integration
```python
import requests

# Send email to extraction API
response = requests.post('http://localhost:5555/extract', 
    json={
        'text': email_content,
        'email_id': 'email_123'
    }
)

data = response.json()
print(f"Found {len(data['entities'])} entities")
```

#### Option B: Batch Processing
```python
from production_hybrid_extractor import ProductionHybridExtractor
import sqlite3

extractor = ProductionHybridExtractor()

# Connect to your email database
conn = sqlite3.connect('/home/pricepro2006/CrewAI_Team/data/crewai_enhanced.db')
cursor = conn.cursor()

# Process unprocessed emails
cursor.execute("""
    SELECT id, subject, body_content 
    FROM emails_enhanced 
    WHERE processed = 0 
    LIMIT 100
""")

for email_id, subject, body in cursor.fetchall():
    result = extractor.extract_entities(f"{subject} {body}", email_id)
    
    # Save results
    # ... your code to save entities
```

---

## üìà Understanding Your Results

### Pattern Statistics
Your system discovered:
- **147,685** unique patterns
- **18** pattern categories
- **85.2%** coverage (vs 1.4% before)

### Top Patterns Found
1. **sales4460** (6,064 times) - Likely a system/user ID
2. **insightsurface@tdsynnex** (2,634 times) - Email address
3. **GENERATED:57371** (5,084 times) - System-generated ID
4. **CAS-091284-B0C6Q4** (309 times) - SPA pattern

### Performance Metrics
- Processing speed: **125ms per email**
- Throughput: **8 emails/second**
- Accuracy: **89.8% F1 score**

---

## üõ†Ô∏è Advanced Usage

### Custom Pattern Rules
Add your own verified patterns:

```python
# Add to td_synnex_pattern_rules.py
custom_patterns = {
    'my_pattern': {
        'patterns': [
            (r'\bMYPATTERN-\d{6}\b', 'CUSTOM', 1.00),
        ]
    }
}
```

### Workflow Automation
```python
def process_quote_to_order(email_text):
    result = extractor.extract_entities(email_text)
    
    if result.workflow == 'quote_to_order':
        quotes = [e.value for e in result.entities if e.classification == 'quote']
        pos = [e.value for e in result.entities if e.classification == 'purchase_order']
        
        print(f"Quote {quotes[0]} converted to PO {pos[0]}")
        # Trigger your order processing system
```

### Export for Analysis
```python
# Export all results to Excel/CSV
import pandas as pd

results = []
for email in emails:
    result = extractor.extract_entities(email)
    for entity in result.entities:
        results.append({
            'email_id': result.email_id,
            'entity': entity.value,
            'type': entity.type,
            'confidence': entity.confidence,
            'purpose': result.purpose
        })

df = pd.DataFrame(results)
df.to_excel('extraction_results.xlsx', index=False)
```

---

## üîç Monitoring & Metrics

### Check System Health
```bash
# View metrics
curl http://localhost:5555/metrics

# Monitor in real-time
python3 monitor_dashboard.py
```

### Database Queries
```sql
-- See pattern distribution
SELECT pattern_type, COUNT(*) as count 
FROM discovered_patterns 
GROUP BY pattern_type 
ORDER BY count DESC;

-- Find high-confidence patterns
SELECT pattern, confidence, occurrences 
FROM discovered_patterns 
WHERE confidence > 0.9 
ORDER BY occurrences DESC 
LIMIT 20;
```

---

## ‚ùì Common Questions

### Q: How do I know if a pattern was found?
Check the `result.entities` list. Each entity has:
- `value`: The actual pattern found
- `type`: Pattern category (ALPHA_NUMERIC, CAS_PATTERN, etc.)
- `confidence`: How sure we are (0.0 to 1.0)
- `source`: Where it came from (verified/discovered/llm)

### Q: What's the difference between verified and discovered patterns?
- **Verified**: Known TD SYNNEX patterns with rules (highest confidence)
- **Discovered**: Patterns found by the discovery process (medium confidence)
- **LLM**: Patterns found by AI when needed (lower confidence)

### Q: How can I improve accuracy?
1. Run human verification to classify more patterns
2. Add custom rules for your specific patterns
3. Enable LLM for complex emails (if you have the model)

### Q: Can it handle new pattern types?
Yes! The system continuously learns. Any pattern it finds gets added to the discovery database.

---

## üö® Troubleshooting

### If extraction seems slow:
```python
# Disable LLM for speed
extractor = ProductionHybridExtractor(config={'use_llm': False})
```

### If patterns are missed:
```python
# Check if pattern exists in discovery
import sqlite3
conn = sqlite3.connect('pattern_extraction.db')
cursor = conn.cursor()
cursor.execute("SELECT * FROM discovered_patterns WHERE pattern LIKE ?", ('%YOUR_PATTERN%',))
print(cursor.fetchall())
```

### To reset and start fresh:
```bash
rm full_discovery/discovery_state.json
rm pattern_extraction.db
python3 deploy_production.py
```

---

## üìû Next Actions

1. **Immediate** (Today):
   - ‚úÖ Run test_system.py to verify it works
   - Review top patterns in discovery report
   - Test with 5-10 real emails

2. **Short-term** (This Week):
   - Run human verification on 100 patterns
   - Integrate with your email pipeline
   - Set up daily batch processing

3. **Long-term** (This Month):
   - Train team on using the system
   - Build dashboards for pattern analytics
   - Implement automated workflows based on patterns

---

## Success Metrics to Track

- **Coverage**: Currently 85.2% - aim for 90%+
- **Processing Speed**: Currently 125ms - maintain under 200ms
- **Human Verification**: Verify 100+ patterns weekly
- **New Pattern Discovery**: Track new patterns found daily

---

**Remember**: The system has already processed all 143,221 emails and found 147,685 patterns. You're starting with a fully trained system - now it's about using and refining it for your specific needs!